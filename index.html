<!DOCTYPE html>
<html>
<head>
<title>Z. Rafii, "A Researcher in Audio," 2020.</title>

<style>
body {
	width: 27cm;
	margin: auto;
	font-size: 16px;
	background-color: DimGray;
}

div.title {
	margin-top: 1.5cm;
	padding-top: 4cm;
	height: 6cm;
	background-color: White;
}

p.title {
	font-size: 20px;
	text-align: center;
	padding-bottom: 0.5cm;
}

div.twocolumn {
	column-count: 2;
	padding-left: 2.5cm;
	column-gap: 1cm;
	padding-right: 2.5cm;
	padding-top: 3cm;
	height: 30cm;
	padding-bottom: 2cm;
	margin-bottom: 0.5cm;
	background-color: White;
	text-align: justify;
}

h4.title {
	margin-top: 1cm;
	margin-bottom: 0.5cm;
	text-align: center;
}

h4.subtitle {
	margin-top: 0.75cm;
	margin-bottom: 0.5cm;
}

p {
	margin: 0cm;
	text-indent: 0.5cm;
}

figure {
	margin-left: 0cm;
	margin-right: 0cm;
	margin-bottom: 0.5cm;
}

img {
	width: 100%;
	margin-bottom: 0.5cm;
}

ul {
	margin-top: 0cm;
	margin-bottom: 0.5cm;
}

table.audio {
	margin-top: 0.5cm;
	margin-bottom: 0.5cm;
	text-align: right;
}

audio {
	width: 7cm;
	height: 1cm;
}

table.bibliography td {
	min-width: 0.5cm;
	margin-top: 0cm;
	padding-bottom: 0.2cm;
	vertical-align: top;
	text-align: justify;
}

</style>
</head>


<body>
<div class="title">
<p class="title"><b>A RESEARCHER IN AUDIO</b></p>
<p class="title"><i>Zafar Rafii</i></p>
<p class="title">PhD in Electrical Engineering &#38; Computer Science<br>
Berkeley, CA, USA<br>
zafarrafii@gmail.com</p>
</div>


<div class="twocolumn" style="padding-top: 0cm; height: 23cm">

<h4 class="title" style="margin-top: 0cm">ABSTRACT</h4>

<p style="text-indent: 0cm">In this website, we present a researcher in audio. The proposed researcher has a PhD in electrical engineering and computer science from Northwestern University, with a focus on audio signal analysis. He has over 30 publications, including conference papers, journal articles, and patents, with more than 1200 citations overall. He is actively involved within the research community, as a reviewer for numerous conferences and journals, a member of the IEEE audio and acoustic signal processing technical committee, and an organizer of networking meetups in the San Francisco Bay Area. He is currently a senior research engineer at Gracenote, where he is working on a number of projects, among others, audio content recognition, audio encoding analysis, and audio beamforming.</p>

<p style="margin-top: 0.5cm"><i><b>Index Termsâ€”</b></i> audio, research, signal processing, source separation, content recognition</p>


<h4 class="title">1. INTRODUCTION</h4>

<figure style="text-align: center">
	<a href="Images/Zafar.jpg"><img src="Images/zafar.jpg" alt="zafar" style="width: 50%"></a>
	<figcaption><b>Fig.1.</b> Overview of the proposed researcher.</figcaption>
</figure>

<p>The proposed researcher is named Zafar Rafii. He received a PhD in electrical engineering and computer science from <a href="http://www.northwestern.edu/">Northwestern University</a> in 2014. He was with the <a href="http://music.cs.northwestern.edu/">Interactive Audio Lab</a>, under the supervision of professor Bryan Pardo. Before that, he was a research engineer at Audionamix, in France. He is now a senior research engineer in the audio group of the Media Technology Lab at <a href="http://www.gracenote.com/">Gracenote</a>.</p>

<p>
The proposed researcher has interest and expertise in audio signal analysis, somewhere between signal processing, machine learning, and cognitive science. He has worked on a number of projects, including:
<ul>
	<li>Blind source separation</li>
	<li>Digital audio effects</li>
	<li>Audio fingerprinting</li>
	<li>Cover song identification</li>
	<li>Audio encoding analysis</li>
	<li>Spatial source separation</li>
	<li>Audio beamforming</li>
	<li>Audio watermarking</li>
	<li>Onset/beat detection</li>
	<li>Audio/video segmentation</li>
</ul>
</p>

<p>
For more information on the proposed researcher, the reader is referred to the following materials:
<ul>
	<li><a href="Zafar Rafii - CV.pdf">CV</a></li>
	<li><a href="https://github.com/zafarrafii">GitHub</a></li>
	<li><a href="https://www.linkedin.com/in/zafarrafii/">LinkedIn</a></li>
	<li><a href="https://scholar.google.com/citations?user=8wbS2EsAAAAJ&hl=en">Google Scholar</a></li>
</ul>
</p>

<p>
For other relevant information related to the proposed researcher, such as the meetups he organizes, the mentoring program he is involved in, or the audio dataset he created, the reader is referred to the following links:
<ul>
	<li><a href="https://www.meetup.com/bishbash/">SF-BISH Bash meetup</a></li>
	<li><a href="https://www.youtube.com/channel/UCfVTmVY__IObKq06vZFryxA">BISH Bash YouTube channel</a></li>
	<li><a href="https://wimir.wordpress.com/mentoring-program/">Women in Music Information Retrieval</a></li>
	<li><a href="https://sigsep.github.io/">Open resources for music source separation</a></li>
</ul>
</p>

<p>The rest of the website is organized as follows. In Section <a href="#research">2</a>, we present a selection of projects the proposed researcher has worked on. In Section <a href="#repet">3</a>, we introduce his PhD thesis work on the REpeating Pattern Extraction Technique (REPET) for blind source separation. In Section <a href="#codes">4</a>, we share links to his GitHub repositories where some of his source codes reside. In Section <a href="#references">5</a>, we provide references to all of his publications, presentations, and other materials.</p>

</div>


<div class="twocolumn">

<h4 class="title" style="margin-top: 0cm" id="research">2. RESEARCH</h4>

<h4 class="subtitle" style="margin-top: 0cm">2.1. Adaptive Reverberation Effects (2008)</h4>

<figure>
	<a href="Images/Reverberation.jpg"><img src="Images/reverberation.jpg" alt="reverberation"></a>
	<figcaption><b>Fig.2.</b> A user rating a sound modified by a series of reverberation settings as to how well it fits her/his audio concept of "boomy" she/he had in mind.</figcaption>
</figure>

<p>People often think about sound in terms of subjective concepts which do not necessarily have known mappings onto the controls of existing audio tools. For example, a bass player may wish to use a reverberation effect to make her/his bass sound more "boomy", but unfortunately there is no "boomy" knob to be found. We developed a system that can quickly learn an audio concept from a user (e.g., a "boomy" effect) and generate a simple controller than can manipulate sounds in terms of that audio concept (e.g., make a sound more "boomy"), bypassing the bottleneck of technical knowledge of complex interfaces and individual differences in subjective terms.</p>

<p>For this study, we focused on reverberation effects. We developed a digital reverberator, mapping the parameters of the digital filters to measures of the reverberation effect, so that the reverberator can be controlled through meaningful descriptors such as "reverberation time" or "spectral centroid." In the learning process, a sound is first modified by a series of reverberation settings using the reverberator. The user then listens and rates each modified sound as to how well it fits the audio concept she/he has in mind. The ratings are finally mapped onto the controls of the reverberator and a simple controller is built with which the user is able to manipulate the degree of her/his audio concept on a sound. Several experiments conducted on human subjects showed that the system learns quickly (under 3 minutes), predicts user responses well (mean correlation of 0.75), and meets users' expectations (average human rating of 7.4 out of 10).</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2009">[32]</a>, <a href="#sabin2011">[14]</a>, and <a href="#rafii20092">[35]</a>.</p>


<h4 class="subtitle" style="margin-top: 0cm">2.2. DUET using the CQT (2011)</h4>

<figure>
	<a href="Images/Duet.png"><img src="Images/duet.png" alt="duet"></a>
	<figcaption><b>Fig.3.</b> Blind separation of a stereo recording of Homer, Bart, and Lisa using DUET.</figcaption>
</figure>

<p>The Degenerate Unmixing Estimation Technique (DUET) is a blind source separation method that can separate an arbitrary number of unknown sources using a single stereo mixture. DUET builds a two-dimensional histogram from the amplitude ratio and phase difference between channels, where each peak indicates a source, with peak location corresponding to the mixing parameters associated with that source. Provided that the time-frequency bins of the sources do not overlap too much - an assumption generally validated by speech mixtures, DUET partitions the time-frequency representation of the mixture by assigning each bin to the source with the closest mixing parameters. However, when time-frequency bins of the sources start to overlap more - as generally seen in music mixtures when using the common short-time Fourier transform (STFT), peaks start to fuse in the 2d histogram and DUET cannot perform separation effectively.</p>

<p>We proposed to improve peak/source separation in DUET by building the 2d histogram from an alternative time-frequency representation based on the constant-Q transform (CQT). Unlike the Fourier transform, the CQT has a logarithmic frequency resolution, mirroring the human auditory system and matching the geometrically spaced frequencies of the Western music scale, therefore better adapted to music mixtures. We also proposed other contributions to enhance DUET, such as adaptive boundaries for the 2d histogram to improve peak resolving when sources are spatially too close to each other, and Wiener filtering to improve source reconstruction. Experiments on mixtures of piano notes and harmonic sources showed that peak/source separation is overall improved, especially at low octaves (under 200 Hz) and for small mixing angles (under pi/6 rad).</p>


</div>

<div class="twocolumn">

<p>Unlike the classic DUET based on the Fourier transform, DUET combined with the CQT can resolve adjacent pitches in low octaves as well as in high octaves thanks to the log frequency resolution of the CQT:</p>

<table class="audio">
	<tr><td>Mixture of 3 piano notes</td><td><audio controls><source src="Audio/DUET/piano_mixture.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated A2</td><td><audio controls><source src="Audio/DUET/A2_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated Bb2</td><td><audio controls><source src="Audio/DUET/Bb2_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated B2</td><td><audio controls><source src="Audio/DUET/B2_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original A2</td><td><audio controls><source src="Audio/DUET/A2_original.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original Bb2</td><td><audio controls><source src="Audio/DUET/Bb2_original.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original B2</td><td><audio controls><source src="Audio/DUET/B2_original.mp3" type="audio/mpeg"></audio></td></tr>
</table>

<p>DUET combined with the CQT and adaptive boundaries helps to improve separation when sources have low pitches (for example, here, between the two cellos) and/or are spatially too close to each other:</p>

<table class="audio">
	<tr><td>Mixture of 4 instruments</td><td><audio controls><source src="Audio/DUET/instruments_mixture.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated cello 1</td><td><audio controls><source src="Audio/DUET/cello1_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated cello 2</td><td><audio controls><source src="Audio/DUET/cello2_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated flute</td><td><audio controls><source src="Audio/DUET/flute_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated strings</td><td><audio controls><source src="Audio/DUET/strings_estimated.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original cello 1</td><td><audio controls><source src="Audio/DUET/cello1_original.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original cello 2</td><td><audio controls><source src="Audio/DUET/cello2_original.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original flute</td><td><audio controls><source src="Audio/DUET/flute_original.mp3" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original strings</td><td><audio controls><source src="Audio/DUET/strings_original.mp3" type="audio/mpeg"></audio></td></tr>
</table>

<p>For more information about this project, the reader is referred to <a href="#rafii2011">[31]</a>.</p>


<h4 class="subtitle" style="margin-top: 0cm">2.3. Live Music Fingerprinting (2014)</h4>

<figure>
	<a href="Images/Thresholding.png"><img src="Images/thresholding.png" alt="thresholding"></a>
	<figcaption><b>Fig.4.</b> Overview of the fingerprinting stage. The audio signal is first transformed into a log-frequency spectrogram by using the CQT. The CQT-based spectrogram is then transformed into a binary image by using an adaptive thresholding method.</figcaption>
</figure>

<p>Suppose that you are at a music festival checking on an artist, and you would like to quickly know about the song that is being played (e.g., title, lyrics, album, etc.). If you have a smartphone, you could record a sample of the live performance and compare it against a database of existing recordings from the artist. Services such as Shazam or SoundHound will not work here, as this is not the typical framework for audio fingerprinting or query-by-humming systems, as a live performance is neither identical to its studio version (e.g., variations in instrumentation, key, tempo, etc.) nor it is a hummed or sung melody. We propose an audio fingerprinting system that can deal with live version identification by using image processing techniques. Compact fingerprints are derived using a log-frequency spectrogram and an adaptive thresholding method, and template matching is performed using the Hamming similarity and the Hough Transform.</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2014">[24]</a>.</p>

<br/>
<br/>
<br/>
<br/>

</div>


<div class="twocolumn">

<h4 class="subtitle" style="margin-top: 0cm">2.4. Lossy Audio Compression Identification (2018)</h4>

<figure>
	<a href="Images/compression.png"><img src="Images/compression.png" alt="compression"></a>
	<figcaption><b>Fig.5.</b> Results for an audio example encoded with AC-3. The system identified traces of compression corresponding to AC-3, but not to other lossy coding formats such as MP3, AAC, Vorbis, or WMA.</figcaption>
</figure>

<p>We propose a system which can estimate from an audio recording that has previously undergone lossy compression the parameters used for the encoding, and therefore identify the corresponding lossy coding format. The system analyzes the audio signal and searches for the compression parameters and framing conditions which match those used for the encoding. In particular, we propose a new metric for measuring traces of compression which is robust to variations in the audio content and a new method for combining the estimates from multiple audio blocks which can refine the results. We evaluated this system with audio excerpts from songs and movies, compressed into various coding
formats, using different bit rates, and captured digitally as well as through analog transfer. Results showed that our system can
identify the correct format in almost all cases, even at high bitrates and with distorted audio, with an overall accuracy of 0.96.</p>

<p>For more information about this project, the reader is referred to <a href="#kim2018">[15]</a>.</p>

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

<h4 class="subtitle" style="margin-top: 0cm">2.5. Sliding DFT with Kernel Windowing (2018)</h4>

<figure>
	<a href="Images/Kernels.png"><img src="Images/kernels.png" alt="kernels"></a>
	<figcaption><b>Fig.6.</b> Kernels derived from the (a) Hanning, (b) Blackman, (c) triangular, (d) Parzen, (e) Gaussian (with <i>&alpha;</i> = 2.5), and (f) Kaiser (with <i>&beta;</i> = 0.5) windows. The kernels were derived for an <i>N</i>-point DFT where <i>N</i> = 2,048 samples. Only the first 100 coefficients at the bottom-left corner of the <i>N</i>-by-<i>N</i>
kernels are shown. The values are displayed in log of amplitude.</figcaption>
</figure>

<p>The sliding discrete Fourier transform (SDFT) is an efficient method for computing the N-point DFT of a given signal starting at a given sample from the N-point DFT of the same signal starting at the previous sample. However, the SDFT does not allow the use of a window function, generally incorporated in the computation of the DFT to reduce spectral leakage, as it would break its sliding property. We show how windowing can be included in the SDFT by using a kernel derived from the window function, while keeping the process computationally efficient. In addition, this approach allows for turning other transforms, such as the modified discrete cosine transform (MDCT), into efficient sliding versions of themselves.</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2018">[9]</a>.</p>

</div>


<div class="twocolumn">

<h4 class="title" style="margin-top: 0cm" id="repet">3. REPET</h4>

<figure>
	<a href="Images/Repet.png"><img src="Images/repet.png" alt="repet"></a>
	<figcaption><b>Fig.7.</b> Overview of REPET.</figcaption>
</figure>

<p>Repetition is a fundamental element in generating and perceiving structure. In audio, mixtures are often composed of structures where a repeating background signal is superimposed with a varying foreground signal. On this basis, we present the REpeating Pattern Extraction Technique (REPET), a simple approach for separating the repeating background from the non-repeating foreground in an audio mixture. The basic idea is to find the repeating elements in the mixture, derive the underlying repeating models, and extract the repeating background by comparing the models to the mixture. Unlike other separation approaches, REPET does not depend on special parameterizations, does not rely on complex frameworks, and does not require external information. Because it is only based on repetition, it has the advantage of being simple, fast, blind, and therefore completely and easily automatable.</p>

<h4 class="subtitle">3.1 Original REPET (2011)</h4>

<figure>
	<a href="Images/repet_original_overview.png"><img src="Images/repet_original_overview.png" alt="repet_original_overview"></a>
	<figcaption><b>Fig.8.</b> Overview of the original REPET. <i>Stage 1</i>: calculation of the beat spectrum <i>b</i> and estimation of a repeating period <i>p</i>. <i>Stage 2</i>: segmentation of the mixture spectrogram <i>V</i> and calculation of the repeating segment <i>S</i>. <i>Stage 3</i>: calculation of the repeating spectrogram <i>W</i> and derivation of the time-frequency mask <i>M</i>.</figcaption>
</figure>

<p>The original REPET aims at identifying and extracting the repeating patterns in an audio mixture, by estimating a period of the underlying repeating structure and modeling a segment of the periodically repeating background.</p>

<p>Experiments on a data set of song clips showed that REPET can be effectively applied for music/voice separation. Experiments also showed that REPET can be combined with other methods to improve background/foreground separation; for example, it can be used as a preprocessor to pitch detection algorithms to improve melody extraction, or as a postprocessor to a singing voice separation algorithm to improve music/voice separation.</p>

<p>REPET can be easily extended to handle varying repeating structures, by simply applying the method along time, on individual segments or via a sliding window. Experiments on a data set of full-track real-world songs showed that this method can be effectively applied for music/voice separation.</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2011">[30]</a>, <a href="#rafii2013">[13]</a>, and <a href="#rafii20142">[34]</a>.</p>

<table class="audio">
	<tr><td>Mixture</td><td><audio controls><source src="Audio/REPET/dev1__tamy-que_pena_tanto_faz__snip_6_19__mix.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated background</td><td><audio controls><source src="Audio/REPET/dev1__tamy-que_pena_tanto_faz__snip_6_19__mix_background.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated foreground</td><td><audio controls><source src="Audio/REPET/dev1__tamy-que_pena_tanto_faz__snip_6_19__mix_foreground.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original accompaniment</td><td><audio controls><source src="Audio/REPET/dev1__tamy-que_pena_tanto_faz__snip_6_19__guitar.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original vocals</td><td><audio controls><source src="Audio/REPET/dev1__tamy-que_pena_tanto_faz__snip_6_19__vocals.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/repet_original_example.png"><img src="Images/repet_original_example.png" alt="repet_original_example"></a>
	<figcaption><b>Fig.9.</b> Music/voice separation using REPET. The mixture is a female singer (foreground) singing over a guitar accompaniment (background). The guitar has a repeating chord progression that is stable along the song. The spectrograms and the mask are shown for 5 seconds and up to 2.5 kHz.</figcaption>
</figure>

</div>


<div class="twocolumn">

<h4 class="subtitle" style="margin-top: 0cm">3.2 Adaptive REPET (2012)</h4>

<figure>
	<a href="Images/repet_adaptive_overview.png"><img src="Images/repet_adaptive_overview.png" alt="repet_adaptive_overview"></a>
	<figcaption><b>Fig.10.</b> Overview of the adaptive REPET. <i>Stage 1</i>: calculation of the beat spectrogram <i>B</i> and estimation of the repeating periods <i>p<sub>j</sub></i>â€™s. <i>Stage 2</i>: filtering of the mixture spectrogram <i>V</i> and calculation of an initial repeating spectrogram <i>U</i>. <i>Stage 3</i>: calculation of the refined repeating spectrogram <i>W</i> and derivation of the time-frequency mask <i>M</i>.</figcaption>
</figure>

<p>The original REPET works well when the repeating background is relatively stable (e.g., a verse or the chorus in a song); however, the repeating background can also vary over time (e.g., a verse followed by the chorus in the song). The adaptive REPET is an extension of the original REPET that can handle varying repeating structures, by estimating the time-varying repeating periods and extracting the repeating background locally, without the need for segmentation or windowing.</p>

<p>Experiments on a data set of full-track real-world songs showed that the adaptive REPET can be effectively applied for music/voice separation.</p>

<p>For more information about this project, the reader is referred to <a href="#liutkus2012">[28]</a> and <a href="#rafii20142">[34]</a>.</p>

<table class="audio">
	<tr><td>Mixture</td><td><audio controls><source src="Audio/REPET/dev2__another_dreamer-the_ones_we_love__snip_69_94__mix.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated background</td><td><audio controls><source src="Audio/REPET/dev2__another_dreamer-the_ones_we_love__snip_69_94__mix_background.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated foreground</td><td><audio controls><source src="Audio/REPET/dev2__another_dreamer-the_ones_we_love__snip_69_94__mix_foreground.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original accompaniment</td><td><audio controls><source src="Audio/REPET/dev2__another_dreamer-the_ones_we_love__snip_69_94__mix-vocals.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original vocals</td><td><audio controls><source src="Audio/REPET/dev2__another_dreamer-the_ones_we_love__snip_69_94__vocals.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/repet_adaptive_example.png"><img src="Images/repet_adaptive_example.png" alt="repet_adaptive_example"></a>
	<figcaption><b>Fig.11.</b> Music/voice separation using the adaptive REPET. The mixture is a male singer (foreground) singing over a guitar and drums accompaniment (background). The guitar has a repeating chord progression that changes around 15 seconds. The spectrograms and the mask are shown for 5 seconds and up to 2.5 kHz.</figcaption>
</figure>


<h4 class="subtitle">3.3 REPET-SIM (2012)</h4>

<figure>
	<a href="Images/repet_sim_overview.png"><img src="Images/repet_sim_overview.png" alt="repet_sim_overview"></a>
	<figcaption><b>Fig.12.</b> Overview of REPET-SIM. <i>Stage 1</i>: calculation of the similarity matrix <i>S</i> and estimation of the repeating indices <i>j<sub>k</sub></i>â€™s. <i>Stage 2</i>: filtering of the mixture spectrogram <i>V</i> and calculation of an initial repeating spectrogram <i>U</i>. <i>Stage 3</i>: calculation of the refined repeating spectrogram <i>W</i> and derivation of the time-frequency mask <i>M</i>.</figcaption>
</figure>

<p>The REPET methods work well when the repeating background has periodically repeating patterns (e.g., jackhammer noise); however, the repeating patterns can also happen intermittently or without a global or local periodicity (e.g., frogs by a pond). REPET-SIM is a generalization of REPET that can also handle non-periodically repeating structures, by using a similarity matrix to identify the repeating elements.</p>

</div>

<div class="twocolumn">

<p>Experiments on a data set of full-track real-world songs showed that REPET-SIM can be effectively applied for music/voice separation.</p>

<p>REPET-SIM can be easily implemented online to handle real-time computing, particularly for real-time speech enhancement. The online REPET-SIM simply processes the time frames of the mixture one after the other given a buffer that temporally stores past frames. Experiments on a data set of two-channel mixtures of one speech source and real-world background noise showed that the online REPET-SIM can be effectively applied for real-time speech enhancement.</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2012">[27]</a>, <a href="#rafii20132">[26]</a>, and <a href="#rafii20142">[34]</a>.</p>

<table class="audio">
	<tr><td>Mixture</td><td><audio controls><source src="Audio/REPET/dev_Sq1_Co_B_mix.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated foreground</td><td><audio controls><source src="Audio/REPET/dev_Sq1_Co_B_mix_foreground.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Estimated background</td><td><audio controls><source src="Audio/REPET/dev_Sq1_Co_B_mix_background.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original speech</td><td><audio controls><source src="Audio/REPET/dev_Sq1_Co_B_sim.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original noise</td><td><audio controls><source src="Audio/REPET/dev_Sq1_Co_B_noi.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/repet_sim_example.png"><img src="Images/repet_sim_example.png" alt="repet_sim_example"></a>
	<figcaption><b>Fig.13.</b> Noise/speech separation using REPET-SIM. The mixture is a female speaker (foreground) speaking in a town square (background). The square has repeating noisy elements (passers-by and cars) that happen intermittently. The spectrograms and the mask are shown for 5 seconds and up to 2 kHz.</figcaption>
</figure>

<br/>
<br/>
<br/>
<br/>
<br/>


<h4 class="subtitle" style="margin-top: 0cm">3.4 uREPET (2015)</h4>

<p>Repetition is a fundamental element in generating and perceiving structure in audio. Especially in music, structures tend to be composed of patterns that repeat through time (e.g., rhythmic elements in a musical accompaniment), and also frequency (e.g., different notes of the same instrument). The auditory system has the remarkable ability to parse such patterns by identifying repetitions within the audio mixture. On this basis, we propose a simple user interface system for recovering patterns repeating in time and frequency in mixtures of sounds. A user selects a region in the log-frequency spectrogram of an audio recording from which she/he wishes to recover a repeating pattern covered by an undesired element (e.g., a note covered by a cough). The selected region is then cross-correlated with the spectrogram to identify similar regions where the underlying pattern repeats. The identified regions are finally averaged over their repetitions and the repeating pattern is recovered.</p>

<p>For more information about this project, the reader is referred to <a href="#rafii2015">[20]</a>.</p>

<table class="audio">
	<tr><td>Melody covered by a cough</td><td><audio controls><source src="Audio/uREPET/melody&cough.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered melody</td><td><audio controls><source src="Audio/uREPET/melody-cough.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original melody</td><td><audio controls><source src="Audio/uREPET/melody.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original cough</td><td><audio controls><source src="Audio/uREPET/cough.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/urepet_example1a.png"><img src="Images/urepet_example1a.png" alt="urepet_example1a"></a>
	<figcaption><b>Fig.14.</b> Log-spectrogram of a melody with a cough covering the first note. The user selected the region of the cough (solid line) and the system identified similar regions where the underlying note repeats (dashed lines).</figcaption>
</figure>

<br/>
<br/>

</div>

<div class="twocolumn">

<figure>
	<a href="Images/urepet_example1b.png"><img src="Images/urepet_example1b.png" alt="urepet_example1b"></a>
	<figcaption><b>Fig.15.</b> Log-spectrogram of the melody with the first note recovered. The system averaged the identified regions over their repetitions and filtered out the cough from the selected region.</figcaption>
</figure>

<table class="audio">
	<tr><td>Accompaniment covered by vocals</td><td><audio controls><source src="Audio/uREPET/accompaniment&vocals.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered accompaniment</td><td><audio controls><source src="Audio/uREPET/accompaniment-vocals.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original accompaniment</td><td><audio controls><source src="Audio/uREPET/accompaniment.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original vocals</td><td><audio controls><source src="Audio/uREPET/vocals.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/urepet_example2a.png"><img src="Images/urepet_example2a.png" alt="urepet_example2a"></a>
	<figcaption><b>Fig.16.</b> Log-spectrogram of a song with vocals covering an accompaniment. The user selected the region of the first measure (solid line) and the system identified similar regions where the underlying accompaniment repeats (dashed lines).</figcaption>
</figure>

<figure>
	<a href="Images/urepet_example2b.png"><img src="Images/urepet_example2b.png" alt="urepet_example2b"></a>
	<figcaption><b>Fig.17.</b> Log-spectrogram of the song with the first measure of the accompaniment recovered. The system averaged the identified regions over their repetitions and filtered out the vocals from the selected region.</figcaption>
</figure>

<table class="audio">
	<tr><td>Speech covering a noise</td><td><audio controls><source src="Audio/uREPET/speech&noise.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered speech</td><td><audio controls><source src="Audio/uREPET/speech-noise.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original speech</td><td><audio controls><source src="Audio/uREPET/speech.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original noise</td><td><audio controls><source src="Audio/uREPET/noise.wav" type="audio/mpeg"></audio></td></tr>
</table>

<figure>
	<a href="Images/urepet_example3a.png"><img src="Images/urepet_example3a.png" alt="urepet_example3a"></a>
	<figcaption><b>Fig.18.</b> Log-spectrogram of a speech covering a noise. The user selected the region of the first sentence (solid line) and the system identified similar regions where the underlying noise repeats (dashed lines).</figcaption>
</figure>

</div>

<div class="twocolumn">

<figure>
	<a href="Images/urepet_example3b.png"><img src="Images/urepet_example3b.png" alt="urepet_example3b"></a>
	<figcaption><b>Fig.19.</b> Log-spectrogram of the first sentence of the speech extracted. The system averaged the identified regions over their repetitions and extracted the speech from the selected region.</figcaption>
</figure>


<h4 class="subtitle">3.5 PROJET-MAG (2017)</h4>

<p>We propose a simple user-assisted method for the recovery of repeating patterns in time and frequency which can occur in mixtures of sounds. Here, the user selects a region in a logfrequency spectrogram from which they seek to recover the underlying pattern which is obscured by another interfering source, such as a chord masked by a cough. A cross-correlation is then performed between the selected region and the spectrogram, revealing similar regions. The most similar region is then selected and a variant on the PROJET algorithm, termed PROJET-MAG, is then used to extract the common time-frequency components from the two regions, as well as extracting the components which are not common to both. The results obtained are compared to another user-assisted method based on REPET, and the PROJET-MAG method is demonstrated to give improved results over this baseline.</p>

<p>For more information about this project, the reader is referred to <a href="#fitzgerald2017">[17]</a>.</p>

<table class="audio">
	<tr><td>Melody covered by a cough</td><td><audio controls><source src="Audio/uREPET/melody&cough.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered melody uREPET</td><td><audio controls><source src="Audio/uREPET/melody-cough (uREPET).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered melody PROJET-MAG</td><td><audio controls><source src="Audio/uREPET/melody-cough (PROJET-MAG).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original melody</td><td><audio controls><source src="Audio/uREPET/melody.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original cough</td><td><audio controls><source src="Audio/uREPET/cough.wav" type="audio/mpeg"></audio></td></tr>
</table>

<table class="audio">
	<tr><td>Accompaniment covered by vocals</td><td><audio controls><source src="Audio/uREPET/accompaniment&vocals.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered acc uREPET</td><td><audio controls><source src="Audio/uREPET/accompaniment-vocals (uREPET).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered acc PROJET-MAG</td><td><audio controls><source src="Audio/uREPET/accompaniment-vocals (PROJET-MAG).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original accompaniment</td><td><audio controls><source src="Audio/uREPET/accompaniment.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original vocals</td><td><audio controls><source src="Audio/uREPET/vocals.wav" type="audio/mpeg"></audio></td></tr>
</table>

<table class="audio">
	<tr><td>Speech covering a noise</td><td><audio controls><source src="Audio/uREPET/speech&noise.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered speech uREPET</td><td><audio controls><source src="Audio/uREPET/speech-noise (uREPET).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Recovered speech PROJET-MAG</td><td><audio controls><source src="Audio/uREPET/speech-noise (PROJET-MAG).wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original speech</td><td><audio controls><source src="Audio/uREPET/speech.wav" type="audio/mpeg"></audio></td></tr>
	<tr><td>Original noise</td><td><audio controls><source src="Audio/uREPET/noise.wav" type="audio/mpeg"></audio></td></tr>
</table>

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

</div>


<div class="twocolumn">

<h4 class="title" style="margin-top: 0cm" id="codes">4. CODES</h4>

<h4 class="subtitle" style="margin-top: 0cm">4.1. Z</h4>

<p>This <a href="https://github.com/zafarrafii/Z">repository</a> includes a Matlab class, a Python module, a Jupyter notebook, and a Julia module which implement/illustrate several methods/functions for audio signal processing.</p> 

<p>
<ul>
	<li><i>stft</i> - Short-time Fourier transform (STFT)</li>
	<li><i>istft</i>  - Inverse STFT</li>
	<li><i>cqtkernel</i>  - Constant-Q transform (CQT) kernel</li>
	<li><i>cqtspectrogram</i>  - CQT spectrogram using a CQT kernel</li>
	<li><i>cqtchromagram</i>  - CQT chromagram using a CQT kernel</li>
	<li><i>mfcc</i>  - Mel frequency cepstrum coefficients (MFCCs)</li>
	<li><i>dct</i>  - Discrete cosine transform (DCT) using the fast Fourier transform (FFT)</li>
	<li><i>dst</i>  - Discrete sine transform (DST) using the FFT</li>
	<li><i>mdct</i>  - Modified discrete cosine transform (MDCT) using the DCT-IV</li>
	<li><i>imdct</i>  - Inverse MDCT using the DCT-IV</li>
</ul>
</p>

<figure>
	<a href="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/stft.png"><img src="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/stft.png" alt="ft-spectrogram" style="width: 50%"></a><a href="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/cqtspectrogram.png"><img src="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/cqtspectrogram.png" alt="cqt-spectrogram" style="width: 50%"></a><a href="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/cqtchromagram.png"><img src="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/cqtchromagram.png" alt="cqt-chromagram" style="width: 50%"></a><a href="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/mfcc.png"><img src="https://raw.githubusercontent.com/zafarrafii/Z/master/images/matlab/mfcc.png" alt="mfcc" style="width: 50%"></a>
	<figcaption><b>Fig.20.</b> FT-spectrogram, CQT-spectrogram, CQT-chromagram, and MFCCs using the Z class.</figcaption>
</figure>


<h4 class="subtitle">4.2. REPET</h4>

<p>This <a href="https://github.com/zafarrafii/REPET">repository</a> includes a Matlab class and a Python module which implement a number of methods/functions for the different algorithms of the REpeating Pattern Extraction Technique (REPET).</p>

<p>
<ul>
	<li><i>original</i> - REPET (original)
	<li><i>extended</i> - REPET extended
	<li><i>adaptive</i> - Adaptive REPET
	<li><i>sim</i> - REPET-SIM
	<li><i>simonline</i> - REPET-SIM online
</ul>
</p>

<p>This <a href="https://github.com/zafarrafii/REPET-GUI">repository</a> includes Matlab GUIs to demo the original REPET and REPET-SIM.</p>

<p>This <a href="https://github.com/zafarrafii/uREPET">repository</a> contains a Matlab GUI for uREPET, a simple user interface system for recovering patterns repeating in time and frequency in mixtures of sounds.</p>

<figure>
	<a href="https://github.com/zafarrafii/uREPET/raw/master/images/select_urepet_background_undo_save.gif"><img src="https://github.com/zafarrafii/uREPET/raw/master/images/select_urepet_background_undo_save.gif" alt="urepet"></a>
	<figcaption><b>Fig.21.</b> uREPET GUI.</figcaption>
</figure>

<p>For more information, the reader is referred to <a href="#rafii2011">[30]</a>, <a href="#liutkus2012">[28]</a>, <a href="#rafii2012">[27]</a>, <a href="#rafii20132">[26]</a>, <a href="#rafii2013">[13]</a>, <a href="#rafii20142">[34]</a>, and <a href="#rafii2015">[20]</a>.</p>


<h4 class="subtitle">4.3. Others</h4>

<p>This <a href="https://github.com/zafarrafii/Zap">repository</a> contains a Matlab GUI which implements Zafar's audio player (Zap), featuring some practical functionalities such as a synchronized spectrogram, a select/drag tool, and a playback line.</p>

<figure>
	<a href="https://raw.githubusercontent.com/zafarrafii/Zap/master/images/zap_play.gif"><img src="https://raw.githubusercontent.com/zafarrafii/Zap/master/images/zap_play.gif"></a>
	<figcaption><b>Fig.21.</b> Zap GUI.</figcaption>
</figure>

<p>This <a href="https://github.com/zafarrafii/Python-Problems">repository</a> contains Jupyter notebooks with Python coding problems (and solutions). These can be good exercises for beginners and more experienced users to improve and review their programming skills in Python. The problems are borrowed from the Internet and the solutions are given in Jupyter notebooks with detailed comments to help understand them. The proposed solutions are not necessarily optimized so feel free to to contact me if you find anything wrong with them.</p>

</div>


<div class="twocolumn">

<h4 style="text-align: center; margin-top: 0cm" id="references">5. REFERENCES</h4>

<h4 class="subtitle" style="margin-top: 0cm">5.1. Patents</h4>

<table class="bibliography">
	<tr><td>[1]</td><td>Rober Coover and Zafar Rafii. "Methods and Apparatus to Fingerprint an Audio Signal via Normalization," 16453654, March 2020. [<a href="http://www.freepatentsonline.com/y2020/0082835.html">url</a>]</td></tr>
	
	<tr><td>[2]</td><td>Zafar Rafii, Markus Cremer, and Bongjun Kim. "Methods and Apparatus to Perform Windowed Sliding Transforms," 15942369, April 2019. [<a href="http://www.freepatentsonline.com/y2019/0122678.html">url</a>]</td></tr>
	
	<tr><td>[3]</td><td>Zafar Rafii, Markus Cremer, and Bongjun Kim. "Methods, Apparatus and Articles of Manufacture to Identify Sources of Network Streaming Services," 15793543, April 2019. [<a href="http://www.freepatentsonline.com/y2019/0122673.html">url</a>]</td></tr>
	
	<tr><td>[4]</td><td>Zafar Rafii. "Methods and Apparatus to Extract a Pitch-independent Timbre Attribute from a Media Signal," 15920060, January 2019. [<a href="http://www.freepatentsonline.com/10186247.html">url</a>]</td></tr>
	
	<tr><td>[5]</td><td>Markus Cremer, Zafar Rafii, Robert Coover, and Prem Seetharaman. "Automated Cover Song Identification," 15698557, July 2018. [<a href="http://www.freepatentsonline.com/y2018/0189390.html">url</a>]</td></tr>
	
	<tr><td>[6]</td><td>Zafar Rafii and Prem Seetharaman. "Audio Identification based on Data Structure," 15698532, March 2018. [<a href="http://www.freepatentsonline.com/y2018/0075140.html">url</a>]</td></tr>
	
	<tr><td>[7]</td><td>Zafar Rafii. "Audio Matching based on Harmonogram," 14980622, July 2016. [<a href="http://www.freepatentsonline.com/y2016/0196343.html">url</a>]</td></tr>
	
	<tr><td>[8]</td>
	<td>Bryan Pardo and Zafar Rafii. "Acoustic Separation System and Method," 13612413, March 2013. [<a href="http://www.freepatentsonline.com/y2013/0064379.html">url</a>]</td></tr>
</table>


<h4 class="subtitle">5.2. Journal Articles</h4>

<table class="bibliography">
	<tr><td id="rafii2018">[9]</td><td>Zafar Rafii. "Sliding Discrete Fourier Transform with Kernel Windowing," <i>IEEE Signal Processing Magazine</i>, vol. 35, no. 6, November 2018. [<a href="Documents/Journals/Rafii - Sliding Discrete Fourier Transform with Kernel Windowing - 2018.pdf">article</a>]</td></tr>
	
	<tr><td>[10]</td><td>Zafar Rafii, Antoine Liutkus, Fabian-Robert St&ouml;ter, Stylianos Ioannis Mimilakis, Derry FitzGerald, and Bryan Pardo. "An Overview of Lead and Accompaniment Separation in Music," <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>, vol. 26, no. 8, August 2018. [<a href="Documents/Journals/Rafii-Liutkus-Stoter-Mimilakis-FitzGerald-Pardo - An Overview of Lead and Accompaniment Separation in Music - 2018.pdf">article</a>]</td></tr>
	
	<tr><td>[11]</td><td>Zafar Rafii, Zhiyao Duan, and Bryan Pardo. "Combining Rhythm-based and Pitch-based Methods for Background and Melody Separation," <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>, vol. 22, no. 12, December 2014. [<a href="Documents/Journals/Rafii-Duan-Pardo - Combining Rhythm-Based and Pitch-Based Methods for Background and Melody Separation - 2014.pdf">article</a>]</td></tr>
	
	<tr><td>[12]</td><td>Antoine Liutkus, Derry FitzGerald, Zafar Rafii, Bryan Pardo, and Laurent Daudet. "Kernel Additive Models for Source Separation," <i>IEEE Transactions on Signal Processing</i>, vol. 62, no. 16, August 2014. [<a href="Documents/Journals/Liutkus-FitzGerald-Rafii-Pardo-Daudet - Kernel Additive Models for Source Separation - 2014.pdf">article</a>]</td></tr>
	
	<tr><td id="rafii2013">[13]</td><td>Zafar Rafii and Bryan Pardo. "REpeating Pattern Extraction Technique (REPET): A Simple Method for Music/Voice Separation," <i>IEEE Transactions on Audio, Speech, and Language Processing</i>, vol. 21, no. 1, January 2013. [<a href="Documents/Journals/Rafii-Pardo - REpeating Pattern Extraction Technique (REPET) A Simple Method for Music-Voice Separation - 2013.pdf">article</a>]</td></tr>
	
	<tr><td id="sabin2011">[14]</td><td>Andrew Todd Sabin, Zafar Rafii, and Bryan Pardo. "Weighting-Function-Based Rapid Mapping of Descriptors to Audio Processing Parameters," <i>Journal of the Audio Engineering Society</i>, vol. 59, no. 6, June 2011. [<a href="Documents/Journals/Sabin-Rafii-Pardo - Weighting-Function-Based Rapid Mapping of Descriptors to Audio Processing Parameters - 2011.pdf">article</a>]</td></tr>
</table>


<h4 class="subtitle">5.3. Conference Articles</h4>

<table class="bibliography">
	<tr><td id="kim2018">[15]</td><td>Bongjun Kim and Zafar Rafii. "Lossy Audio Compression Identification," <i>26<sup>th</sup> European Signal Processing Conference</i>, Rome, Italy, September 3-7, 2018. [<a href="Documents/Conferences/Kim-Rafii - Lossy Audio Compression Identification - 2018.pdf">article</a>][<a href="Documents/Conferences/Kim-Rafii - Lossy Audio Compression Identification - 2018 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td>[16]</td><td>Prem Seetharaman and Zafar Rafii. "Cover Song Identification with 2d Fourier Transform Sequences," <i>42<sup>nd</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, New Orleans, USA, March 5-9, 2017. [<a href="Documents/Conferences/Seetharaman-Rafii - Cover Song Identification with 2D Fourier Transform Sequences - 2017.pdf">article</a>][<a href="Documents/Conferences/Seetharaman-Rafii - Cover Song Identification with 2D Fourier Transform Sequences - 2017 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="fitzgerald2017">[17]</td><td>Derry FitzGerald, Zafar Rafii, and Antoine Liutkus. "User Assisted Separation of Repeating Patterns in Time and Frequency using Magnitude Projections," <i>42<sup>nd</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, New Orleans, USA, March 5-9, 2017. [<a href="Documents/Conferences/FitzGerald-Rafii-Liutkus - User Assisted Separation of Repeating Patterns in Time and Frequency using Magnitude Projections - 2017.pdf">article</a>][<a href="Documents/Conferences/FitzGerald-Rafii-Liutkus - User Assisted Separation of Repeating Patterns in Time and Frequency using Magnitude Projections - 2017 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td>[18]</td><td>Antoine Liutkus, Fabian-Robert St&ouml;ter, Zafar Rafii, Daichi Kitamura, Bertrand Rivet, Nobutaka Ito, Nobutaka Ono, and Lujie Fontecave.  "The 2016 Signal Separation Evaluation Campaign," <i>13<sup>th</sup> International Conference on Latent Variable Analysis and Signal Separation</i>, Grenoble, France, February 21-23, 2017. [<a href="Documents/Conferences/Liutkus-Stoter-Rafii-Kitamura-Rivet-Ito-Ono-Fontecave - The 2016 Signal Separation Evaluation Campaign - 2017.pdf">article</a>]</td></tr>
	
	<tr><td>[19]</td><td>Nobutaka Ono, Zafar Rafii, Daichi Kitamura, Nobutaka Ito, and Antoine Liutkus. "The 2015 Signal Separation Evaluation Campaign," <i>12<sup>th</sup> International Conference on Latent Variable Analysis and Signal Separation</i>, Liberec, Czech Republic, August 25-28, 2015. [<a href="Documents/Conferences/Ono-Rafii-Kitamura-Ito-Liutkus - The 2015 Signal Separation Evaluation Campaign - 2015.pdf">article</a>]</td></tr>
	
	<tr><td id="rafii2015">[20]</td><td>Zafar Rafii, Antoine Liutkus, and Bryan Pardo. "A Simple User Interface System for Recovering Patterns Repeating in Time and Frequency in Mixtures of Sounds," <i>40<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Brisbane, Australia, April 19-24, 2015. [<a href="Documents/Conferences/Rafii-Liutkus-Pardo - A Simple User Interface System for Recovering Patterns Repeating in Time and Frequency in Mixtures of Sounds - 2015.pdf">article</a>][<a href="Documents/Conferences/Rafii-Liutkus-Pardo - A Simple User Interface System for Recovering Patterns Repeating in Time and Frequency in Mixtures of Sounds - 2015 (poster).pdf">poster</a>]</td></tr>
</table>

</div>

<div class="twocolumn">

<table class="bibliography">
	<tr><td>[21]</td><td>Antoine Liutkus, Derry FitzGerald, and Zafar Rafii. "Scalable Audio Separation with Light Kernel Additive Modelling," <i>40<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Brisbane, Australia, April 19-24, 2015. [<a href="Documents/Conferences/Liutkus-FitzGerald-Rafii - Scalable Audio Separation with Light Kernel Additive Modelling - 2015.pdf">article</a>][<a href="Documents/Conferences/Liutkus-FitzGerald-Rafii - Scalable Audio Separation with Light Kernel Additive Modelling - 2015 (slides).pdf">slides</a>]</td></tr>

	<tr><td>[22]</td><td>Derry FitzGerald, Antoine Liutkus, Zafar Rafii, Bryan Pardo, and Laurent Daudet. "Harmonic/Percussive Separation using Kernel Additive Modelling," <i>25<sup>th</sup> IET Irish Signals and Systems Conference</i>, Limerick, Ireland, June 26-27, 2014. [<a href="Documents/Conferences/FitzGerald-Liutkus-Rafii-Pardo-Daudet - Harmonic-Percussive Separation using Kernel Additive Modelling - 2014.pdf">article</a>]</td></tr>

	<tr><td>[23]</td><td>Antoine Liutkus, Zafar Rafii, Bryan Pardo, Derry FitzGerald, and Laurent Daudet. "Kernel Spectrogram Models for Source Separation," <i>4<sup>th</sup> Joint Workshop on Hands-free Speech Communication Microphone Arrays</i>, Nancy, France, May 12-14, 2014. [<a href="Documents/Conferences/Liutkus-Rafii-Pardo-FitzGerald-Daudet - Kernel Spectrogram Models for Source Separation - 2014.pdf">article</a>][<a href="Documents/Conferences/Liutkus-Rafii-Pardo-FitzGerald-Daudet - Kernel Spectrogram Models for Source Separation - 2014 (slides).pdf">slides</a>]</td></tr>
	
	<tr><td id="rafii2014">[24]</td><td>Zafar Rafii, Bob Coover, and Jinyu Han. "An Audio Fingerprinting System for Live Version Identification using Image Processing Techniques," <i>39<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Florence, Italy, May 4-9, 2014. [<a href="Documents/Conferences/Rafii-Coover-Han - An Audio Fingerprinting System for Live Version Identification using Image Processing Techniques - 2014.pdf">article</a>][<a href="Documents/Conferences/Rafii-Coover-Han - An Audio Fingerprinting System for Live Version Identification using Image Processing Techniques - 2014 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td>[25]</td><td>Zafar Rafii, Francois G. Germain, Dennis L. Sun, and Gautham J. Mysore. "Combining Modeling of Singing Voice and Background Music for Automatic Separation of Musical Mixtures," <i>14<sup>th</sup> International Society for Music Information Retrieval</i>, Curutiba, PR, Brazil, November 4-8, 2013. [<a href="Documents/Conferences/Rafii-Germain-Sun-Mysore - Combining Modeling of Singing Voice and Background Music for Automatic Separation of Musical Mixtures - 2013.pdf">article</a>][<a href="Documents/Conferences/Rafii-Germain-Sun-Mysore - Combining Modeling of Singing Voice and Background Music for Automatic Separation of Musical Mixtures - 2013 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="rafii20132">[26]</td><td>Zafar Rafii and Bryan Pardo. "Online REPET-SIM for Real-time Speech Enhancement," <i>38<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Vancouver, BC, Canada, May 26-31, 2013. [<a href="Documents/Conferences/Rafii-Pardo - Online REPET-SIM for Real-time Speech Enhancement - 2013.pdf">article</a>][<a href="Documents/Conferences/Rafii-Pardo - Online REPET-SIM for Real-time Speech Enhancement - 2013 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="rafii2012">[27]</td><td>Zafar Rafii and Bryan Pardo. "Music/Voice Separation using the Similarity Matrix," <i>13<sup>th</sup> International Society for Music Information Retrieval</i>, Porto, Portugal, October 8-12, 2012. [<a href="Documents/Conferences/Rafii-Pardo - Music-Voice Separation using the Similarity Matrix - 2012.pdf">article</a>][<a href="Documents/Conferences/Rafii-Pardo - Music-Voice Separation using the Similarity Matrix - 2012 (slides).pdf">slides</a>]</td></tr>
	
	<tr><td id="liutkus2012">[28]</td><td>Antoine Liutkus, Zafar Rafii, Roland Badeau, Bryan Pardo, and Ga&euml;l Richard. "Adaptive Filtering for Music/Voice Separation Exploiting the Repeating Musical Structure," <i>37<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Kyoto, Japan, March 25-30, 2012. [<a href="Documents/Conferences/Liutkus-Rafii-Badeau-Pardo-Richard - Adaptive Filtering for Music-Voice Separation Exploiting the Repeating Musical Structure - 2012.pdf">article</a>][<a href="Documents/Conferences/Liutkus-Rafii-Badeau-Pardo-Richard - Adaptive Filtering for Music-Voice Separation Exploiting the Repeating Musical Structure - 2012 (slides).pdf">slides</a>]</td></tr>
	
	<tr><td>[29]</td><td>Mark Cartwright, Zafar Rafii, Jinyu Han, and Bryan Pardo. "Making Searchable Melodies: Human vs. Machine," <i>3<sup>rd</sup> Human Computation Workshop</i>, San Francisco, CA, USA, August 8, 2011. [<a href="Documents/Conferences/Cartwright-Rafii-Han-Pardo - Making Searchable Melodies Human vs. Machine - 2011.pdf">article</a>][<a href="Documents/Conferences/Cartwright-Rafii-Han-Pardo - Making Searchable Melodies Human vs. Machine - 2011 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="rafii2011">[30]</td><td>Zafar Rafii and Bryan Pardo. "A Simple Music/Voice Separation Method based on the Extraction of the Repeating Musical Structure," <i>36<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Prague, Czech Republic, May 22-27, 2011. [<a href="Documents/Conferences/Rafii-Pardo - A Simple Music-Voice Separation Method based on the Extraction of the Repeating Musical Structure - 2011.pdf">article</a>][<a href="Documents/Conferences/Rafii-Pardo - A Simple Music-Voice Separation Method based on the Extraction of the Repeating Musical Structure - 2011 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="rafii2011">[31]</td><td>Zafar Rafii and Bryan Pardo. "Degenerate Unmixing Estimation Technique using the Constant Q Transform," <i>36<sup>th</sup> IEEE International Conference on Acoustics, Speech and Signal Processing</i>, Prague, Czech Republic, May 22-27, 2011. [<a href="Documents/Conferences/Rafii-Pardo - Degenerate Unmixing Estimation Technique using the Constant Q Transform - 2011.pdf">article</a>][<a href="Documents/Conferences/Rafii-Pardo - Degenerate Unmixing Estimation Technique using the Constant Q Transform - 2011 (poster).pdf">poster</a>]</td></tr>
	
	<tr><td id="rafii2009">[32]</td><td>Zafar Rafii and Bryan Pardo. "Learning to control a Reverberator using Subjective Perceptual Descriptors," <i>10<sup>th</sup> International Society for Music Information Retrieval</i>, Kobe, Japan, October 26-30 2009. [<a href="Documents/Conferences/Rafii-Pardo - Learning to Control a Reverberator using Subjective Perceptual Descriptors - 2009.pdf">article</a>][<a href="Documents/Conferences/Rafii-Pardo - Learning to Control a Reverberator using Subjective Perceptual Descriptors - 2009 (poster).pdf">poster</a>]</td></tr>
</table>


<h4 class="subtitle">5.4. Book Chapters</h4>

<table class="bibliography">
	<tr><td>[33]</td><td>Bryan Pardo, Zafar Rafii, and Zhiyao Duan. "Audio Source Separation in a Musical Context," <i>Handbook of Systematic Musicology</i>, Springer, Berlin, Heidelberg, 2018. [<a href="Documents/Other/Pardo-Rafii-Duan - Audio Source Separation in a Musical Context - 2018.pdf">article</a>]</td></tr>
	
	<tr><td id="rafii20142">[34]</td><td>Zafar Rafii, Antoine Liutkus, and Bryan Pardo. "REPET for Background/Foreground Separation in Audio," <i>Blind Source Separation</i>, Springer, Berlin, Heidelberg, 2014. [<a href="Documents/Other/Rafii-Liutkus-Pardo - REPET for Background-Foreground Separation in Audio - 2014.pdf">article</a>]</td></tr>
</table>


<h4 class="subtitle">5.5. Technical Reports</h4>

<table class="bibliography">
	<tr><td id="rafii20092">[35]</td><td>Zafar Rafii and Bryan Pardo. "A Digital Reverberator controlled through Measures of the Reverberation," Northwestern University, EECS Department Technical Report, NWU-EECS-09-08, 2009. [<a href="Documents/Other/Rafii-Pardo - A Digital Reverberator controlled through Measures of the Reverberation - 2009.pdf">article</a>]</td></tr>
</table>

<h4>5.6. Tutorials</h4>

<table class="bibliography">
	<tr><td>[36]</td><td>Josh McDermott, Bryan Pardo, and Zafar Rafii. "Leveraging Repetition to Parse the Auditory Scene," <i>13<sup>th</sup> International Society for Music Information Retrieval</i>, Porto, Portugal, October 8-12, 2012. [<a href="Documents/Presentations/McDermott-Pardo-Rafii - Leveraging Repetition to Parse the Auditory Scene - 2012.pdf">slides</a>]</td></tr>
</table>


<h4 class="subtitle">5.7. Talks</h4>

<table class="bibliography">
	<tr><td>[37]</td><td>Zafar Rafii. "Identifying Video Sources by Identifying Audio Compression," T&eacute;l&eacute;com ParisTech, Paris, France, April 13, 2018. [<a href="Documents/Presentations/Rafii - Identifying Video Sources by Identifying Audio Compression - 2018.pdf">slides</a>]</td></tr>
</table>

</div>

<div class="twocolumn">

<table class="bibliography">
	<tr><td>[38]</td><td>Zafar Rafii. "Source Separation by Repetition," Center for Computer Research in Music and Acoustics, Stanford University, Stanford, CA, USA, July 15, 2015.</td></tr>
	
	<tr><td>[39]</td><td>Zafar Rafii. "Source Separation by Repetition," Center for New Music and Audio Technologies, University of California at Berkeley, Berkeley, CA, USA, May 11, 2015.</td></tr>
	
	<tr><td>[40]</td><td>Zafar Rafii. "An Audio Fingerprinting System for Live Version Identification using Image Processing Techniques," <i>Midwest Music Information Retrieval Gathering</i>, Northwestern University, Evanston, IL, USA, June 14, 2014. [<a href="Documents/Presentations/Rafii - An Audio Fingerprinting System for Live Version Identification using Image Processing Techniques - 2014.pdf">slides</a>]</td></tr>
	
	<tr><td>[41]</td><td>Zafar Rafii. "A Simple Music/Voice Separation Method based on the Extraction of the Repeating Musical Structure," T&eacute;l&eacute;com ParisTech, Paris, France, July 29, 2011. </td></tr>
	
	<tr><td>[42]</td><td>Zafar Rafii. "REPET," <i>Midwest Music Information Retrieval Gathering</i>, Northwestern University, Evanston, IL, USA, June 24, 2011. [<a href="Documents/Presentations/Rafii - REPET - 2011.pdf">slides</a>]</td></tr>
	
	<tr><td>[43]</td><td>Zafar Rafii, Raphael Blouet, and Antoine Liutkus. "Discriminant within Non-negative Matrix Factorization for Musical Components Recognition," <i>DMRN+2: Digital Music Research Network One-day Workshop 2007</i>, Queen Mary, University of London, London, UK, December 18, 2007. [<a href="Documents/Presentations/Rafii-Blouet-Liutkus - Discriminant Approach within Non-negative Matrix Factorization for Musical Components Recognition - 2007.pdf">poster</a>]</td></tr>
</table>

<h4 class="subtitle">5.8. Lectures</h4>

<table class="bibliography">
	<tr><td>[44]</td><td>Zafar Rafii. "Audio Fingerprinting," EECS 352: Machine Perception of Music and Audio, Northwestern University, 2014. [<a href="Documents/Lectures/Rafii - Audio Fingerprinting - 2014.pdf">slides</a>]</td></tr>
	
	<tr><td>[45]</td><td>Zafar Rafii. "REpeating Pattern Extraction Technique (REPET)," EECS 352: Machine Perception of Music and Audio, Northwestern University, 2014. [<a href="Documents/Lectures/Rafii - REPET - 2014.pdf">slides</a>]</td></tr>
	
	<tr><td>[46]</td><td>Zafar Rafii. "Rhythm Analysis in Music," EECS 352: Machine Perception of Music and Audio, Northwestern University, 2014. [<a href="Documents/Lectures/Rafii - Rhythm Analysis in Music - 2014.pdf">slides</a>]</td></tr>
	
	<tr><td>[47]</td><td>Zafar Rafii. "Time-frequency Masking," EECS 352: Machine Perception of Music and Audio, Northwestern University, 2014. [<a href="Documents/Lectures/Rafii - Time-frequency Masking - 2014.pdf">slides</a>]</td></tr>
</table>

<h4 class="subtitle">5.9. Data Sets</h2>

<table class="bibliography">
	<tr><td>[48]</td><td>Zafar Rafii, Antoine Liutkus, Fabian-Robert St&ouml;ter, Stylianos Ioannis Mimilakis, and Rachel Bittner. "MUSDB18-HQ â€“ an uncompressed version of MUSDB18," 2019. [<a href="https://doi.org/10.5281/zenodo.3338373">url</a>]</td></tr>
	
	<tr><td>[49]</td><td>Zafar Rafii, Antoine Liutkus, Fabian-Robert St&ouml;ter, Stylianos Ioannis Mimilakis, and Rachel Bittner. "The MUSDB18 corpus for music separation," 2017. [<a href="https://doi.org/10.5281/zenodo.1117372">url]</a></td></tr>
</table>

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

</div>

</body>

</html>